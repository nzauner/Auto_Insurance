{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto Insurance Analysis\n",
    "\n",
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Goals\n",
    "\n",
    "- Analyze auto insurance data.\n",
    "- Build a logistic regression model to predict crash probability for auto insurance customers.\n",
    "- Build a linear regression model to predict crash cost for auto insurance customers.\n",
    "- Use model results to develop crash percentage and assign customers to new risk profiles.\n",
    "- Determine cost of premiums based on risk profiles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Data\n",
    "\n",
    "The dataset for this project contains 6043 records of auto insurance data. Each record\n",
    "represents a customer at an auto insurance company. Using this data, we will be able to ascertain what\n",
    "influences the likelihood of a car crash. Then subsequently, we will be able to determine the cost to resolve a claim. The data in this project is the typical type of corporate data you would receive from a company in the insurance field-- a typical flat file from client records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "%run ../python_files/imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import and Data Examination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import auto insurance data\n",
    "auto_df = pd.read_csv('../data/auto_insurance_data.csv')\n",
    "\n",
    "# change column names to lower-case\n",
    "auto_df.columns = [i.lower() for i in auto_df.columns]\n",
    "\n",
    "# quick overview of the dataset\n",
    "auto_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After a quick overview of the dataset, we see that we are working with 6043 total observations and 25 different variables. The response variable we will be using is Crash, which indicates whether a car was in a crash or not. The remaining 24 variables will be used as explanatory variables. We also notice a good mix of continuous and categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick review of the variables in the dataset\n",
    "auto_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For modeling purposes, we know that we will have to convert all categorical variables to dummy variables. As we can see above, there are 10 categorical variables that will need to go through this conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# quick review of the characteristics of our current continuous variables in the dataset\n",
    "auto_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice above that there is a large range between some of our observations. However, it is not appropriate to dismiss these as outliers, as we do not want to skew or create bias within our dataset. Also, above we cannot view the descriptions of our 10 categorical variables until we convert them to continous variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the number of NaN values in the dataset\n",
    "auto_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, we see above that our dataset does not contain any missing values, so we will not need to worry about imputation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Data Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dummy values for the categorical variables\n",
    "\n",
    "auto_df['mstatus'] = auto_df['mstatus'].map({'Yes': 1, 'No': 0})\n",
    "auto_df['sex'] = auto_df['sex'].map({'M': 1, 'F': 0})\n",
    "auto_df['parent1'] = auto_df['parent1'].map({'Yes': 1, 'No': 0})\n",
    "auto_df['red_car'] = auto_df['red_car'].map({'yes': 1, 'no': 0})\n",
    "auto_df['revoked'] = auto_df['revoked'].map({'Yes': 1, 'No': 0})\n",
    "auto_df['urbanicity'] = auto_df['urbanicity'].map({'Highly Urban/ Urban': 1, 'Highly Rural': 0})\n",
    "auto_df['education'] = auto_df['education'].map({'<High School': 0, 'High School': 0, 'Bachelors': 1, 'Masters': 1, 'PhD': 1})\n",
    "auto_df['job'] = auto_df['job'].map({'Student': 1, 'Blue Collar': 0, 'Clerical': 0, 'Doctor': 0, 'Home Maker': 0, 'Lawyer': 0, 'Manager': 0, 'Professional': 0})\n",
    "auto_df['car_use'] = auto_df['car_use'].map({'Commercial': 1, 'Private': 0})\n",
    "auto_df['car_type'] = auto_df['car_type'].map({'Sports Car': 1, 'SUV': 1, 'Minivan': 1, 'Pickup': 0, 'Van': 0, 'Panel Truck': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log Transformations for non-normalized variables. Drop the original variable from the dataset.\n",
    "\n",
    "def log_col(df, col):\n",
    "    '''Convert column to log values and\n",
    "    drop the original column\n",
    "    '''\n",
    "    df[f'{col}_log'] = np.log(df[col])\n",
    "    df.drop(col, axis=1, inplace=True)\n",
    "\n",
    "log_col(auto_df, 'tif')\n",
    "log_col(auto_df, 'bluebook')\n",
    "log_col(auto_df, 'travtime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# quick review of the characteristics of all variables in the dataset, \n",
    "# including the new dummy variables and log-transformed variables\n",
    "auto_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Train and Test Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split auto_insurance_df into train and test datasets for our logistic and linear regression models\n",
    "\n",
    "#'features' will be used in both models\n",
    "features = auto_df.drop(['crash', 'crash_cost'], axis = 1)\n",
    "\n",
    "#train and test datasets for logistic regression model\n",
    "crash = auto_df['crash']\n",
    "x_train_log, x_test_log, y_train_log, y_test_log = train_test_split(features, crash, test_size = 0.2, random_state = 10)\n",
    "\n",
    "#train and test datasets for simple linear regression model\n",
    "crash_cost = auto_df['crash_cost']\n",
    "x_train_lin, x_test_lin, y_train_lin, y_test_lin = train_test_split(features, crash_cost, test_size = 0.2, random_state = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlations for logistic regression model\n",
    "x_train_log.corr(method = 'pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Correlation Heatmap for logistic regression model\n",
    "\n",
    "mask = np.zeros_like(x_train_log.corr())\n",
    "triangle_indices = np.triu_indices_from(mask)\n",
    "mask[triangle_indices] = True\n",
    "\n",
    "plt.figure(figsize=(35,30))\n",
    "ax = sns.heatmap(x_train_log.corr(method='pearson'), cmap=\"coolwarm\", mask=mask, annot=True, annot_kws={\"size\": 18}, square=True, linewidths=4)\n",
    "sns.set_style('white')\n",
    "plt.xticks(fontsize=14, rotation=45)\n",
    "plt.yticks(fontsize=14, rotation=0)\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "#plt.ylabel(ylabel=' ', labelpad=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlations for simple linear regression model\n",
    "x_train_lin.corr(method = 'pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation Heatmap for simple linear regression model\n",
    "\n",
    "mask = np.zeros_like(x_train_lin.corr())\n",
    "triangle_indices = np.triu_indices_from(mask)\n",
    "mask[triangle_indices] = True\n",
    "\n",
    "plt.figure(figsize=(35,30))\n",
    "ax = sns.heatmap(x_train_lin.corr(method='pearson'), cmap=\"coolwarm\", mask=mask, annot=True, annot_kws={\"size\": 18}, square=True, linewidths=4)\n",
    "sns.set_style('white')\n",
    "plt.xticks(fontsize=14, rotation=45)\n",
    "plt.yticks(fontsize=14, rotation=0)\n",
    "bottom, top = ax.get_ylim()\n",
    "ax.set_ylim(bottom + 0.5, top - 0.5)\n",
    "#plt.ylabel(ylabel=' ', labelpad=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive Feature Elimination for Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_model = LogisticRegression()\n",
    "rfecv_log = RFECV(estimator=logreg_model, step=1, cv=StratifiedKFold(10), scoring='accuracy')\n",
    "rfecv_log.fit(x_train_log, y_train_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_importance_log = list(zip(features, rfecv_log.support_))\n",
    "new_features_log = []\n",
    "for key,value in enumerate(feature_importance_log):\n",
    "    if(value[1]) == True:\n",
    "        new_features_log.append(value[0])\n",
    "        \n",
    "print(new_features_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive Feature Elimination for Simple Linear Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg_model = LinearRegression()\n",
    "rfecv_lin = RFECV(estimator=linreg_model, step=1, scoring='r2')\n",
    "rfecv_lin.fit(x_train_lin, y_train_lin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_importance_lin = list(zip(features, rfecv_lin.support_))\n",
    "new_features_lin = []\n",
    "for key,value in enumerate(feature_importance_lin):\n",
    "    if(value[1]) == True:\n",
    "        new_features_lin.append(value[0])\n",
    "        \n",
    "print(new_features_lin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Train and Test Datasets after Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final train and test datasets for logistic regression model\n",
    "x_train_log = x_train_log[new_features_log]\n",
    "x_test_log = x_test_log[new_features_log]\n",
    "\n",
    "#final train and test datasets for simple linear regression model\n",
    "x_train_lin = x_train_lin[new_features_lin]\n",
    "x_test_lin = x_test_lin[new_features_lin]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train_log.shape)\n",
    "print(x_test_log.shape)\n",
    "print(x_train_lin.shape)\n",
    "print(x_test_lin.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
